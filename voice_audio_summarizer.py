# -*- coding: utf-8 -*-
"""Voice/Audio_summarizer.ipynb

Automatically generated by Colab.

"""

!pip install -q transformers torchaudio gradio accelerate
import torch
import torchaudio
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import gradio as gr

# Load models
device = "cuda" if torch.cuda.is_available() else "cpu"

# Transcription model
processor = WhisperProcessor.from_pretrained("openai/whisper-small")
asr_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small").to(device)

# Summarization model
summ_model_name = "philschmid/bart-large-cnn-samsum"
tokenizer = AutoTokenizer.from_pretrained(summ_model_name)
summarizer = AutoModelForSeq2SeqLM.from_pretrained(summ_model_name).to(device)

# Utility functions
def transcribe_audio_chunk(waveform, sample_rate):
    waveform = torchaudio.functional.resample(waveform, orig_freq=sample_rate, new_freq=16000)
    input_features = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors="pt").input_features.to(device)
    predicted_ids = asr_model.generate(input_features)
    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription

def summarize_text(text):
    inputs = tokenizer([text], max_length=1024, return_tensors="pt", truncation=True).to(device)
    summary_ids = summarizer.generate(inputs["input_ids"], max_length=200, min_length=20, length_penalty=2.0, num_beams=4)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

def split_audio(file_path, chunk_duration=30):
    waveform, sample_rate = torchaudio.load(file_path)
    chunk_samples = chunk_duration * sample_rate
    chunks = []

    for i in range(0, waveform.shape[1], int(chunk_samples)):
        chunk = waveform[:, i:i + int(chunk_samples)]
        if chunk.shape[1] > 0:
            chunks.append((chunk, sample_rate))

    return chunks

# Gradio interface logic
def process_audio(file_path):
    all_transcripts = []
    all_summaries = []

    chunks = split_audio(file_path, chunk_duration=30)

    for chunk, sample_rate in chunks:
        transcript = transcribe_audio_chunk(chunk, sample_rate)
        summary = summarize_text(transcript)
        all_transcripts.append(transcript)
        all_summaries.append(summary)

    full_conversation = " ".join(all_transcripts)
    final_summary = summarize_text(full_conversation)

    # Format transcript and summaries for display
    chunk_outputs = ""
    for i, (t, s) in enumerate(zip(all_transcripts, all_summaries)):
        chunk_outputs += f"**Chunk {i+1} Transcript:** {t}\n\nğŸ“„ **Summary:** {s}\n\n"

    return chunk_outputs.strip(), final_summary

# UI
inputs = gr.Audio(type="filepath", label="ğŸ¤ Upload or Record Audio File")
outputs = [
    gr.Markdown(label="All Chunks Transcript + Summary"),
    gr.Textbox(label="Final Full Audio Summary")
]

app = gr.Interface(fn=process_audio, inputs=inputs, outputs=outputs, title="ğŸ—£ï¸ Voice Summarizer (30s Chunks)")

app.launch()

